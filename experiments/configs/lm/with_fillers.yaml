dataset: wikitext
dataset_subset: wikitext-103-v1
base_model: distilgpt2
num_epochs: 20  # We have 1.5x data with fillers, so we should train for 1.5x less epochs to equalize training compute
batch_size: 4
eval_steps: 250
gradient_accumulation_steps: 32
lr: 0.0001
weight_decay: 0.1
warmup_ratio: 0.05
filler_to_token_ratio: 1.0
no_fillers_prob: 0.5