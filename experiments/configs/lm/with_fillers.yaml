dataset: wikitext
dataset_subset: wikitext-103-v1
base_model: distilgpt2
num_epochs: 30
batch_size: 8
eval_steps: 250
gradient_accumulation_steps: 16
lr: 0.0001
weight_decay: 0.1
warmup_ratio: 0.05
filler_to_token_ratio: 1.0
no_fillers_prob: 0.5