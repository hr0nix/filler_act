dataset: wikitext
dataset_subset: wikitext-103-v1
base_model: distilgpt2
num_epochs: 15  # We have 2x data with fillers, so we should train for 2x less epochs to equalize training compute
batch_size: 8
eval_steps: 250
gradient_accumulation_steps: 16
lr: 0.0001
weight_decay: 0.1
warmup_ratio: 0.05
filler_prob: 0.5