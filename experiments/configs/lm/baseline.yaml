dataset: wikitext
dataset_subset: wikitext-103-v1
base_model: distilgpt2
num_epochs: 20
batch_size: 64
eval_steps: 1000
gradient_accumulation_steps: 1
lr: 0.0001
weight_decay: 0.1
warmup_ratio: 0.05
filler_to_token_ratio: 0.0